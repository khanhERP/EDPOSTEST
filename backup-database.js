import { Pool } from 'pg';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// C·∫•u h√¨nh database connection
const pool = new Pool({
  connectionString: process.env.DATABASE_URL,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

async function backupDatabase() {
  const client = await pool.connect();
  const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
  const backupDir = path.join(__dirname, 'database-backup');
  const backupFile = path.join(backupDir, `backup-${timestamp}.sql`);

  try {
    // T·∫°o th∆∞ m·ª•c backup n·∫øu ch∆∞a c√≥
    if (!fs.existsSync(backupDir)) {
      fs.mkdirSync(backupDir, { recursive: true });
    }

    console.log('üîÑ B·∫Øt ƒë·∫ßu backup database...');

    let sqlDump = `-- Database Backup Generated on ${new Date().toISOString()}\n\n`;

    // Danh s√°ch c√°c b·∫£ng c·∫ßn backup
    const tables = [
      'categories',
      'products',
      'employees',
      'tables',
      'orders',
      'order_items',
      'transactions',
      'transaction_items',
      'attendance_records',
      'store_settings',
      'suppliers',
      'customers',
      'point_transactions',
      'inventory_transactions',
      'einvoice_connections',
      'invoice_templates',
      'invoices',
      'invoice_items'
    ];

    // Backup structure v√† data cho t·ª´ng b·∫£ng
    for (const table of tables) {
      try {
        console.log(`üìã Backup b·∫£ng: ${table}`);

        // L·∫•y data t·ª´ b·∫£ng
        const dataResult = await client.query(`SELECT * FROM ${table}`);

        if (dataResult.rows.length > 0) {
          sqlDump += `\n-- Table: ${table}\n`;
          sqlDump += `DROP TABLE IF EXISTS ${table} CASCADE;\n`;

          // T·∫°o INSERT statements
          const columns = Object.keys(dataResult.rows[0]);
          const columnList = columns.join(', ');

          sqlDump += `-- Data for table: ${table}\n`;

          for (const row of dataResult.rows) {
            const values = columns.map(col => {
              const value = row[col];
              if (value === null) return 'NULL';
              if (typeof value === 'string') return `'${value.replace(/'/g, "''")}'`;
              if (value instanceof Date) return `'${value.toISOString()}'`;
              return value;
            }).join(', ');

            sqlDump += `INSERT INTO ${table} (${columnList}) VALUES (${values});\n`;
          }

          sqlDump += '\n';
        }

      } catch (error) {
        console.log(`‚ö†Ô∏è Kh√¥ng th·ªÉ backup b·∫£ng ${table}:`, error.message);
      }
    }

    // Ghi file backup
    fs.writeFileSync(backupFile, sqlDump, 'utf8');

    console.log('‚úÖ Backup ho√†n th√†nh!');
    console.log(`üìÅ File backup: ${backupFile}`);
    console.log(`üìä K√≠ch th∆∞·ªõc: ${(fs.statSync(backupFile).size / 1024 / 1024).toFixed(2)} MB`);

    // T·∫°o file JSON backup cho d·ªÖ ƒë·ªçc
    const jsonBackupFile = path.join(backupDir, `backup-${timestamp}.json`);
    const jsonData = {};

    for (const table of tables) {
      try {
        const result = await client.query(`SELECT * FROM ${table}`);
        jsonData[table] = result.rows;
      } catch (error) {
        console.log(`‚ö†Ô∏è Kh√¥ng th·ªÉ backup JSON cho b·∫£ng ${table}`);
      }
    }

    fs.writeFileSync(jsonBackupFile, JSON.stringify(jsonData, null, 2), 'utf8');
    console.log(`üìÑ File JSON backup: ${jsonBackupFile}`);

    return { sqlFile: backupFile, jsonFile: jsonBackupFile };

  } catch (error) {
    console.error('‚ùå L·ªói backup:', error);
    throw error;
  } finally {
    client.release();
    await pool.end();
  }
}

// Ch·∫°y backup n·∫øu script ƒë∆∞·ª£c g·ªçi tr·ª±c ti·∫øp
if (import.meta.url === `file://${process.argv[1]}`) {
  backupDatabase().catch(console.error);
}

export { backupDatabase };